{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# X Mark Detection using GLIP\n",
    "\n",
    "## Overview\n",
    "This notebook uses GLIP (Grounded Language-Image Pre-training) to detect x marks in video frames.\n",
    "GLIP is a vision-language model that can detect objects based on text descriptions.\n",
    "\n",
    "## Advantages of GLIP\n",
    "- **No manual feature engineering**: Uses deep learning to understand visual patterns\n",
    "- **Text-based queries**: Simply describe what to detect (\"x mark\", \"cross\", \"printed x\")\n",
    "- **Robust to variations**: Handles different lighting, angles, and occlusions\n",
    "- **Pre-trained**: Leverages knowledge from large-scale training data\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install torch torchvision transformers opencv-python matplotlib pillow\n",
    "pip install groundingdino-py  # GLIP/GroundingDINO implementation\n",
    "```\n",
    "\n",
    "Note: If groundingdino-py is not available, you can use the Hugging Face transformers implementation or the official GroundingDINO repository."
   ]
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:09:32.440035Z",
     "start_time": "2025-12-27T14:09:32.418569Z"
    }
   },
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "\n",
    "# Set up matplotlib\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.9.1\n",
      "OpenCV version: 4.12.0\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "load_model",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:09:34.560865Z",
     "start_time": "2025-12-27T14:09:32.450190Z"
    }
   },
   "source": [
    "# Load GLIP/GroundingDINO Model\n",
    "# Option 1: Using transformers (if available)\n",
    "try:\n",
    "    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "    \n",
    "    model_id = \"IDEA-Research/grounding-dino-tiny\"  # Or \"IDEA-Research/grounding-dino-base\"\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Loaded model: {model_id}\")\n",
    "    print(f\"  Model on device: {next(model.parameters()).device}\")\n",
    "    USE_TRANSFORMERS = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"transformers library not available or model not found\")\n",
    "    print(\"Please install: pip install transformers\")\n",
    "    print(\"\\nAlternatively, you can use the official GroundingDINO repository:\")\n",
    "    print(\"git clone https://github.com/IDEA-Research/GroundingDINO.git\")\n",
    "    USE_TRANSFORMERS = False\n",
    "    raise"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model: IDEA-Research/grounding-dino-tiny\n",
      "  Model on device: cpu\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "config",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:09:34.583669Z",
     "start_time": "2025-12-27T14:09:34.576492Z"
    }
   },
   "source": [
    "# Configuration\n",
    "VIDEO_PATH = \"../../videos/example.mp4\"\n",
    "OUTPUT_CSV = \"glip_x_detection_results.csv\"\n",
    "\n",
    "# Detection parameters\n",
    "TEXT_PROMPT = \"x mark . cross . printed x\"  # Text description (use periods to separate multiple terms)\n",
    "CONFIDENCE_THRESHOLD = 0.43  # Minimum confidence score (0-1)\n",
    "SAMPLE_EVERY_N_FRAMES = 30  # Process every Nth frame (30 = 1 frame per second at 30fps)\n",
    "\n",
    "# Processing parameters\n",
    "MAX_IMAGE_SIZE = 800  # Resize frames to this size (maintains aspect ratio)\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Text prompt: '{TEXT_PROMPT}'\")\n",
    "print(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"  Sampling rate: every {SAMPLE_EVERY_N_FRAMES} frames\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Text prompt: 'x mark . cross . printed x'\n",
      "  Confidence threshold: 0.38\n",
      "  Sampling rate: every 30 frames\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "video_metadata",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:09:34.626626Z",
     "start_time": "2025-12-27T14:09:34.586823Z"
    }
   },
   "source": [
    "# Load video metadata\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration_sec = total_frames / fps\n",
    "cap.release()\n",
    "\n",
    "frames_to_process = total_frames // SAMPLE_EVERY_N_FRAMES\n",
    "\n",
    "print(\"Video Metadata:\")\n",
    "print(f\"  Resolution: {width}x{height}\")\n",
    "print(f\"  FPS: {fps:.2f}\")\n",
    "print(f\"  Total Frames: {total_frames}\")\n",
    "print(f\"  Duration: {duration_sec:.2f} seconds\")\n",
    "print(f\"\\nProcessing:\")\n",
    "print(f\"  Frames to process: ~{frames_to_process}\")\n",
    "print(f\"  Estimated time: {frames_to_process * 0.5:.1f}-{frames_to_process * 2:.1f} seconds (depends on hardware)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Metadata:\n",
      "  Resolution: 1280x720\n",
      "  FPS: 30.00\n",
      "  Total Frames: 31211\n",
      "  Duration: 1040.37 seconds\n",
      "\n",
      "Processing:\n",
      "  Frames to process: ~1040\n",
      "  Estimated time: 520.0-2080.0 seconds (depends on hardware)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "detection_functions",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:09:34.638953Z",
     "start_time": "2025-12-27T14:09:34.627095Z"
    }
   },
   "source": [

    "# Detection Functions\n",

    "\n",

    "def resize_image(image: np.ndarray, max_size: int = 800) -> Tuple[np.ndarray, float]:\n",

    "    \"\"\"\n",

    "    Resize image while maintaining aspect ratio.\n",

    "    Returns: (resized_image, scale_factor)\n",

    "    \"\"\"\n",

    "    h, w = image.shape[:2]\n",

    "    scale = min(max_size / max(h, w), 1.0)  # Don't upscale\n",

    "    \n",

    "    if scale < 1.0:\n",

    "        new_w = int(w * scale)\n",

    "        new_h = int(h * scale)\n",

    "        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",

    "        return resized, scale\n",

    "    \n",

    "    return image, 1.0\n",

    "\n",

    "\n",

    "def detect_x_with_glip(image: np.ndarray, text_prompt: str, \n",

    "                       confidence_threshold: float = 0.25) -> Dict:\n",

    "    \"\"\"\n",

    "    Detect x marks in image using GLIP.\n",

    "    \n",

    "    Returns:\n",

    "        Dictionary containing:\n",

    "        - x_detected: bool\n",

    "        - confidence: float (max confidence if multiple detections)\n",

    "        - num_detections: int\n",

    "        - boxes: list of [x1, y1, x2, y2] in original image coordinates\n",

    "        - scores: list of confidence scores\n",

    "        - labels: list of detected label strings\n",

    "    \"\"\"\n",

    "    # Resize image for faster processing\n",

    "    resized_img, scale = resize_image(image, MAX_IMAGE_SIZE)\n",

    "    \n",

    "    # Convert to PIL Image (RGB)\n",

    "    if len(resized_img.shape) == 2:  # Grayscale\n",

    "        pil_image = Image.fromarray(resized_img).convert('RGB')\n",

    "    else:\n",

    "        pil_image = Image.fromarray(resized_img)\n",

    "    \n",

    "    # Run detection\n",

    "    with torch.no_grad():\n",

    "        inputs = processor(images=pil_image, text=text_prompt, return_tensors=\"pt\").to(device)\n",

    "        outputs = model(**inputs)\n",

    "    \n",

    "    # Post-process results (using correct API)\n",

    "    results = processor.post_process_grounded_object_detection(\n",

    "        outputs,\n",

    "        inputs.input_ids,\n",

    "        target_sizes=[pil_image.size[::-1]]  # (height, width)\n",

    "    )[0]\n",

    "    \n",

    "    # Extract detections\n",

    "    boxes = results['boxes'].cpu().numpy()  # [x1, y1, x2, y2] format\n",

    "    scores = results['scores'].cpu().numpy()\n",

    "    labels = results['labels']\n",

    "    \n",

    "    # Filter by confidence threshold\n",

    "    valid_indices = scores >= confidence_threshold\n",

    "    boxes = boxes[valid_indices]\n",

    "    scores = scores[valid_indices]\n",

    "    labels = [label for i, label in enumerate(labels) if valid_indices[i]]\n",

    "    \n",

    "    # Scale boxes back to original image size\n",

    "    if scale < 1.0 and len(boxes) > 0:\n",

    "        boxes = boxes / scale\n",

    "    \n",

    "    # Prepare result\n",

    "    num_detections = len(boxes)\n",

    "    x_detected = num_detections > 0\n",

    "    max_confidence = float(scores.max()) if x_detected else 0.0\n",

    "    \n",

    "    return {\n",

    "        'x_detected': x_detected,\n",

    "        'confidence': max_confidence,\n",

    "        'num_detections': num_detections,\n",

    "        'boxes': boxes.tolist(),\n",

    "        'scores': scores.tolist(),\n",

    "        'labels': labels\n",

    "    }\n",

    "\n",

    "\n",

    "def visualise_detections(image: np.ndarray, detection_result: Dict) -> np.ndarray:\n",

    "    \"\"\"\n",

    "    Draw detection boxes and labels on image.\n",

    "    \"\"\"\n",

    "    vis_image = image.copy()\n",

    "    \n",

    "    if len(vis_image.shape) == 2:  # Convert grayscale to RGB for visualisation\n",

    "        vis_image = cv2.cvtColor(vis_image, cv2.COLOR_GRAY2RGB)\n",

    "    \n",

    "    if not detection_result['x_detected']:\n",

    "        # No detections - add \"NO X\" label\n",

    "        cv2.putText(vis_image, 'NO X DETECTED', (20, 50),\n",

    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 3)\n",

    "        return vis_image\n",

    "    \n",

    "    # Draw each detection\n",

    "    for box, score, label in zip(detection_result['boxes'], \n",

    "                                  detection_result['scores'], \n",

    "                                  detection_result['labels']):\n",

    "        x1, y1, x2, y2 = map(int, box)\n",

    "        \n",

    "        # Draw bounding box\n",

    "        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",

    "        \n",

    "        # Draw label with confidence\n",

    "        label_text = f\"{label}: {score:.2f}\"\n",

    "        \n",

    "        # Background for text\n",

    "        (text_w, text_h), baseline = cv2.getTextSize(\n",

    "            label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",

    "        )\n",

    "        cv2.rectangle(vis_image, (x1, y1 - text_h - baseline - 5), \n",

    "                     (x1 + text_w, y1), (0, 255, 0), -1)\n",

    "        \n",

    "        # Draw text\n",

    "        cv2.putText(vis_image, label_text, (x1, y1 - 5),\n",

    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",

    "    \n",

    "    # Add summary text\n",

    "    summary = f\"Detected: {detection_result['num_detections']} x mark(s)\"\n",

    "    cv2.putText(vis_image, summary, (20, 50),\n",

    "               cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",

    "    \n",

    "    return vis_image\n",

    "\n",

    "\n",

    "print(\"✓ Detection functions defined\")"

   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "test_sample",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:15:34.304611Z",
     "start_time": "2025-12-27T14:09:34.639443Z"
    }
   },
   "source": [

    "# Test on Sample Frames\n",

    "\n",

    "print(\"Testing GLIP on sample frames...\\n",

    "\")\n",

    "\n",

    "# Sample 5 frames evenly distributed\n",

    "sample_frame_indices = np.linspace(0, total_frames - 1, 100, dtype=int)\n",

    "\n",

    "cap = cv2.VideoCapture(VIDEO_PATH)\n",

    "\n",

    "for idx in sample_frame_indices:\n",

    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",

    "    ret, frame = cap.read()\n",

    "    \n",

    "    if not ret:\n",

    "        continue\n",

    "    \n",

    "    # Convert BGR to RGB\n",

    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",

    "    \n",

    "    # Detect\n",

    "    result = detect_x_with_glip(frame_rgb, TEXT_PROMPT, CONFIDENCE_THRESHOLD)\n",

    "    \n",

    "    # Visualise\n",

    "    vis_frame = visualise_detections(frame_rgb, result)\n",

    "    \n",

    "    # Display\n",

    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",

    "    \n",

    "    axes[0].imshow(frame_rgb)\n",

    "    axes[0].set_title('Original Frame')\n",

    "    axes[0].axis('off')\n",

    "    \n",

    "    axes[1].imshow(vis_frame)\n",

    "    axes[1].set_title('Detection Result')\n",

    "    axes[1].axis('off')\n",

    "    \n",

    "    timestamp = idx / fps\n",

    "    plt.suptitle(f\"Frame {idx} (t={timestamp:.2f}s) - X Detected: {result['x_detected']} (conf={result['confidence']:.3f})\", \n",

    "                fontsize=14)\n",

    "    plt.tight_layout()\n",

    "    plt.show()\n",

    "    \n",

    "    # Print details\n",

    "    print(f\"Frame {idx} (t={timestamp:.2f}s):\")\n",

    "    print(f\"  X Detected: {result['x_detected']}\")\n",

    "    print(f\"  Num Detections: {result['num_detections']}\")\n",

    "    if result['x_detected']:\n",

    "        print(f\"  Max Confidence: {result['confidence']:.3f}\")\n",

    "        for i, (score, label) in enumerate(zip(result['scores'], result['labels'])):\n",

    "            print(f\"    Detection {i+1}: {label} ({score:.3f})\")\n",

    "    print()\n",

    "\n",

    "cap.release()\n",

    "print(\"✓ Sample frame testing complete\")"

   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "process_video",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T14:16:26.531390Z",
     "start_time": "2025-12-27T14:15:34.389616Z"
    }
   },
   "source": [
    "# Process Full Video\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(f\"Processing video: {VIDEO_PATH}\")\n",
    "print(f\"Sampling every {SAMPLE_EVERY_N_FRAMES} frames\\n\")\n",
    "\n",
    "results = []\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "frame_idx = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Progress: \", end='', flush=True)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Sample frames\n",
    "    if frame_idx % SAMPLE_EVERY_N_FRAMES == 0:\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect\n",
    "        result = detect_x_with_glip(frame_rgb, TEXT_PROMPT, CONFIDENCE_THRESHOLD)\n",
    "        \n",
    "        # Store result\n",
    "        timestamp = frame_idx / fps\n",
    "        results.append({\n",
    "            'frame_number': frame_idx,\n",
    "            'timestamp_sec': timestamp,\n",
    "            'x_present': result['x_detected'],\n",
    "            'confidence_score': result['confidence'],\n",
    "            'num_detections': result['num_detections'],\n",
    "            'detection_method': 'GLIP'\n",
    "        })\n",
    "        \n",
    "        # Progress indicator\n",
    "        if len(results) % 10 == 0:\n",
    "            progress = (frame_idx / total_frames) * 100\n",
    "            print(f\"{progress:.1f}%\", end='... ', flush=True)\n",
    "    \n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"100% - Done!\\n\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display summary\n",
    "print(f\"✓ Processing complete in {elapsed_time:.2f} seconds\")\n",
    "print(f\"  Processed {len(results_df)} frames\")\n",
    "print(f\"  Average: {len(results_df)/elapsed_time:.2f} frames/second\\n\")\n",
    "\n",
    "print(\"--- Detection Summary ---\")\n",
    "x_present_count = results_df['x_present'].sum()\n",
    "print(f\"Frames with X detected: {x_present_count} / {len(results_df)} ({x_present_count/len(results_df)*100:.1f}%)\")\n",
    "\n",
    "if x_present_count > 0:\n",
    "    print(f\"Average confidence when X detected: {results_df[results_df['x_present']]['confidence_score'].mean():.3f}\")\n",
    "    print(f\"Average detections per frame (when X present): {results_df[results_df['x_present']]['num_detections'].mean():.2f}\")\n",
    "\n",
    "if x_present_count < len(results_df):\n",
    "    print(f\"Average confidence when X not detected: {results_df[~results_df['x_present']]['confidence_score'].mean():.3f}\")\n",
    "\n",
    "# Show first few results\n",
    "print(\"\\nFirst 10 results:\")\n",
    "print(results_df.head(10).to_string())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ../../videos/example.mp4\n",
      "Sampling every 30 frames\n",
      "\n",
      "Progress: 0.9%... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Detect\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m result = \u001b[43mdetect_x_with_glip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEXT_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIDENCE_THRESHOLD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Store result\u001b[39;00m\n\u001b[32m     30\u001b[39m timestamp = frame_idx / fps\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mdetect_x_with_glip\u001b[39m\u001b[34m(image, text_prompt, confidence_threshold)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     45\u001b[39m     inputs = processor(images=pil_image, text=text_prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Post-process results (using correct API)\u001b[39;00m\n\u001b[32m     49\u001b[39m results = processor.post_process_grounded_object_detection(\n\u001b[32m     50\u001b[39m     outputs,\n\u001b[32m     51\u001b[39m     inputs.input_ids,\n\u001b[32m     52\u001b[39m     target_sizes=[pil_image.size[::-\u001b[32m1\u001b[39m]]  \u001b[38;5;66;03m# (height, width)\u001b[39;00m\n\u001b[32m     53\u001b[39m )[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:2529\u001b[39m, in \u001b[36mGroundingDinoForObjectDetection.forward\u001b[39m\u001b[34m(self, pixel_values, input_ids, token_type_ids, attention_mask, pixel_mask, encoder_outputs, output_attentions, output_hidden_states, return_dict, labels)\u001b[39m\n\u001b[32m   2526\u001b[39m     attention_mask = torch.ones_like(input_ids)\n\u001b[32m   2528\u001b[39m \u001b[38;5;66;03m# First, sent images through Grounding DINO base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2529\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2531\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2541\u001b[39m idx = \u001b[32m5\u001b[39m + (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m) + (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m   2542\u001b[39m enc_text_hidden_state = outputs.encoder_last_hidden_state_text \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:2191\u001b[39m, in \u001b[36mGroundingDinoModel.forward\u001b[39m\u001b[34m(self, pixel_values, input_ids, token_type_ids, attention_mask, pixel_mask, encoder_outputs, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   2188\u001b[39m \u001b[38;5;66;03m# Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder\u001b[39;00m\n\u001b[32m   2189\u001b[39m \u001b[38;5;66;03m# Also provide spatial_shapes, level_start_index and valid_ratios\u001b[39;00m\n\u001b[32m   2190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2191\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mmask_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlvl_pos_embed_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mtext_token_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2204\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2205\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a GroundingDinoEncoderOutput when return_dict=True\u001b[39;00m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, GroundingDinoEncoderOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:1580\u001b[39m, in \u001b[36mGroundingDinoEncoder.forward\u001b[39m\u001b[34m(self, vision_features, vision_attention_mask, vision_position_embedding, spatial_shapes, spatial_shapes_list, level_start_index, valid_ratios, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1577\u001b[39m     encoder_vision_states += (vision_features,)\n\u001b[32m   1578\u001b[39m     encoder_text_states += (text_features,)\n\u001b[32m-> \u001b[39m\u001b[32m1580\u001b[39m (vision_features, text_features), attentions = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1581\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1582\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1584\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m   1596\u001b[39m     all_attn_fused_vision += (attentions[\u001b[32m0\u001b[39m],)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:1143\u001b[39m, in \u001b[36mGroundingDinoEncoderLayer.forward\u001b[39m\u001b[34m(self, vision_features, vision_position_embedding, spatial_shapes, spatial_shapes_list, level_start_index, key_padding_mask, reference_points, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids)\u001b[39m\n\u001b[32m   1130\u001b[39m (vision_features, vision_fused_attn), (text_features, text_fused_attn) = \u001b[38;5;28mself\u001b[39m.fusion_layer(\n\u001b[32m   1131\u001b[39m     vision_features=vision_features,\n\u001b[32m   1132\u001b[39m     text_features=text_features,\n\u001b[32m   1133\u001b[39m     attention_mask_vision=key_padding_mask,\n\u001b[32m   1134\u001b[39m     attention_mask_text=text_attention_mask,\n\u001b[32m   1135\u001b[39m )\n\u001b[32m   1137\u001b[39m (text_features, text_enhanced_attn) = \u001b[38;5;28mself\u001b[39m.text_enhancer_layer(\n\u001b[32m   1138\u001b[39m     hidden_states=text_features,\n\u001b[32m   1139\u001b[39m     attention_masks=~text_self_attention_masks,  \u001b[38;5;66;03m# note we use ~ for mask here\u001b[39;00m\n\u001b[32m   1140\u001b[39m     position_embeddings=(text_position_embedding \u001b[38;5;28;01mif\u001b[39;00m text_position_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1141\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m (vision_features, vision_deformable_attn) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeformable_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   1154\u001b[39m     (vision_features, text_features),\n\u001b[32m   1155\u001b[39m     (vision_fused_attn, text_fused_attn, text_enhanced_attn, vision_deformable_attn),\n\u001b[32m   1156\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:1007\u001b[39m, in \u001b[36mGroundingDinoDeformableLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_embeddings, reference_points, spatial_shapes, spatial_shapes_list, level_start_index, output_attentions)\u001b[39m\n\u001b[32m   1004\u001b[39m residual = hidden_states\n\u001b[32m   1006\u001b[39m \u001b[38;5;66;03m# Apply Multi-scale Deformable Attention Module on the multi-scale feature maps.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1020\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m   1021\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:611\u001b[39m, in \u001b[36mGroundingDinoMultiscaleDeformableAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, position_embeddings, reference_points, spatial_shapes, spatial_shapes_list, level_start_index, output_attentions)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    609\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLast dim of reference_points must be 2 or 4, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference_points.shape[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m(\n\u001b[32m    612\u001b[39m     value,\n\u001b[32m    613\u001b[39m     spatial_shapes,\n\u001b[32m    614\u001b[39m     spatial_shapes_list,\n\u001b[32m    615\u001b[39m     level_start_index,\n\u001b[32m    616\u001b[39m     sampling_locations,\n\u001b[32m    617\u001b[39m     attention_weights,\n\u001b[32m    618\u001b[39m     \u001b[38;5;28mself\u001b[39m.im2col_step,\n\u001b[32m    619\u001b[39m )\n\u001b[32m    621\u001b[39m output = \u001b[38;5;28mself\u001b[39m.output_proj(output)\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output, attention_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IdeaProjects/Data Pipeline/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1951\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1946\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1953\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "visualize_results",
   "metadata": {},
   "source": [

    "# Visualise Results Over Time\n",

    "\n",

    "fig, axes = plt.subplots(3, 1, figsize=(16, 10))\n",

    "\n",

    "# Plot 1: X presence over time\n",

    "axes[0].plot(results_df['timestamp_sec'], results_df['x_present'].astype(int), \n",

    "             'o-', markersize=3, linewidth=0.8)\n",

    "axes[0].fill_between(results_df['timestamp_sec'], 0, results_df['x_present'].astype(int), \n",

    "                      alpha=0.3, label='X Present')\n",

    "axes[0].set_xlabel('Time (seconds)')\n",

    "axes[0].set_ylabel('X Present (1=Yes, 0=No)')\n",

    "axes[0].set_title('X Detection Over Time (GLIP)')\n",

    "axes[0].grid(True, alpha=0.3)\n",

    "axes[0].set_ylim(-0.1, 1.1)\n",

    "axes[0].legend()\n",

    "\n",

    "# Plot 2: Confidence scores over time\n",

    "axes[1].plot(results_df['timestamp_sec'], results_df['confidence_score'], \n",

    "             'o-', markersize=3, linewidth=0.8, alpha=0.7, color='blue')\n",

    "axes[1].axhline(y=CONFIDENCE_THRESHOLD, color='r', linestyle='--', \n",

    "               label=f'Confidence Threshold ({CONFIDENCE_THRESHOLD})')\n",

    "axes[1].set_xlabel('Time (seconds)')\n",

    "axes[1].set_ylabel('Confidence Score')\n",

    "axes[1].set_title('Detection Confidence Over Time')\n",

    "axes[1].grid(True, alpha=0.3)\n",

    "axes[1].legend()\n",

    "\n",

    "# Plot 3: Number of detections per frame\n",

    "axes[2].plot(results_df['timestamp_sec'], results_df['num_detections'], \n",

    "             'o-', markersize=3, linewidth=0.8, alpha=0.7, color='green')\n",

    "axes[2].set_xlabel('Time (seconds)')\n",

    "axes[2].set_ylabel('Number of Detections')\n",

    "axes[2].set_title('Number of X Marks Detected Per Frame')\n",

    "axes[2].grid(True, alpha=0.3)\n",

    "\n",

    "plt.tight_layout()\n",

    "plt.show()\n",

    "\n",

    "print(\"✓ Visualisation complete\")"

   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "save_results",
   "metadata": {},
   "source": [
    "# Save Results to CSV\n",
    "\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✓ Results saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "# Create time ranges summary when X is present\n",
    "x_present_df = results_df[results_df['x_present']].copy()\n",
    "\n",
    "if len(x_present_df) > 0:\n",
    "    # Group consecutive detections (allowing gaps of up to 2 sampling intervals)\n",
    "    gap_threshold = SAMPLE_EVERY_N_FRAMES * 2 / fps  # Convert to seconds\n",
    "    x_present_df['group'] = (x_present_df['timestamp_sec'].diff() > gap_threshold).cumsum()\n",
    "    \n",
    "    time_ranges = []\n",
    "    for group_id in x_present_df['group'].unique():\n",
    "        group_data = x_present_df[x_present_df['group'] == group_id]\n",
    "        start_time = group_data['timestamp_sec'].min()\n",
    "        end_time = group_data['timestamp_sec'].max()\n",
    "        duration = end_time - start_time\n",
    "        avg_confidence = group_data['confidence_score'].mean()\n",
    "        total_detections = group_data['num_detections'].sum()\n",
    "        \n",
    "        time_ranges.append({\n",
    "            'start_sec': start_time,\n",
    "            'end_sec': end_time,\n",
    "            'duration_sec': duration,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'total_detections': total_detections\n",
    "        })\n",
    "    \n",
    "    ranges_df = pd.DataFrame(time_ranges)\n",
    "    print(f\"\\n--- Time Ranges When X is Present ---\")\n",
    "    print(ranges_df.to_string(index=False))\n",
    "    \n",
    "    # Save time ranges\n",
    "    ranges_csv = OUTPUT_CSV.replace('.csv', '_time_ranges.csv')\n",
    "    ranges_df.to_csv(ranges_csv, index=False)\n",
    "    print(f\"\\n✓ Time ranges saved to: {ranges_csv}\")\n",
    "else:\n",
    "    print(\"\\nNo X detected in any frame.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "notes",
   "metadata": {},
   "source": [
    "## Notes and Tips\n",
    "\n",
    "### Adjusting Detection Parameters\n",
    "\n",
    "1. **Text Prompt**: Experiment with different descriptions:\n",
    "   - `\"x mark . cross . printed x\"` - Multiple terms increase recall\n",
    "   - `\"printed x mark on laminate sheet\"` - More specific context\n",
    "   - `\"dark x mark . black cross\"` - Color/appearance descriptors\n",
    "\n",
    "2. **Confidence Threshold**: \n",
    "   - Lower (0.15-0.25): More detections, higher false positives\n",
    "   - Higher (0.30-0.40): Fewer false positives, might miss some x marks\n",
    "\n",
    "3. **Sampling Rate**:\n",
    "   - Process every frame: `SAMPLE_EVERY_N_FRAMES = 1` (slower but complete)\n",
    "   - Process 1 per second: `SAMPLE_EVERY_N_FRAMES = 30` (faster)\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "- **grounding-dino-tiny**: Faster, less accurate (good for quick testing)\n",
    "- **grounding-dino-base**: Slower, more accurate (better for production)\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- Use GPU if available (much faster)\n",
    "- Adjust `MAX_IMAGE_SIZE` to balance speed vs accuracy\n",
    "- Process in batches if you have multiple videos\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **Too many false positives**: Increase confidence threshold or refine text prompt\n",
    "- **Missing detections**: Lower confidence threshold or try different text descriptions\n",
    "- **Slow processing**: Increase sampling rate or reduce image size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}